{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Analysis, Trying to Predict Views: Ryan, Hannah, and Spencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"comm\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'YouTube_Final_Data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "YouTube_df = spark.read.csv(data_file, header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132802"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTube_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: string (nullable = true)\n",
      " |-- likes: string (nullable = true)\n",
      " |-- dislikes: string (nullable = true)\n",
      " |-- comment_count: string (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "YouTube_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+---------------+-----------+--------------------+--------------------+-------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|   video_id|trending_date|               title|  channel_title|category_id|        publish_time|                tags|  views|likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
      "+-----------+-------------+--------------------+---------------+-----------+--------------------+--------------------+-------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|2kyS6SvSYSE|     17.14.11|WE WANT TO TALK A...|   CaseyNeistat|         22|2017-11-13T17:13:...|     SHANtell martin| 748374|57527|    2966|        15954|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
      "|1ZAPwfrtAFY|     17.14.11|The Trump Preside...|LastWeekTonight|         24|2017-11-13T07:30:...|\"last week tonigh...|2418783|97185|    6146|        12703|https://i.ytimg.c...|            False|           False|                 False|One year after th...|\n",
      "+-----------+-------------+--------------------+---------------+-----------+--------------------+--------------------+-------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take the first 5 records\n",
    "YouTube_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the data type. Just making sure this is a dataframe \n",
    "type(YouTube_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning: Time to dig around a little bit and see what we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|   comments_disabled|count(video_id)|\n",
      "+--------------------+---------------+\n",
      "|               FALSE|          78531|\n",
      "|               False|          40316|\n",
      "|                null|          11890|\n",
      "|                TRUE|           1266|\n",
      "|    sports and more.|            117|\n",
      "|             Wiz Kid|              4|\n",
      "| sports and more....|              1|\n",
      "|                True|            633|\n",
      "|           Fida Daar|              3|\n",
      "|            farfalle|             41|\n",
      "+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#here trying to see what the proportion of videos are with comments disabled\n",
    "YouTube_df.groupBy(\"comments_disabled\").agg(F.count(\"video_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|   comments_disabled|count(video_id)|\n",
      "+--------------------+---------------+\n",
      "|               False|         118847|\n",
      "|                null|          11890|\n",
      "|    sports and more.|            117|\n",
      "|             Wiz Kid|              4|\n",
      "| sports and more....|              1|\n",
      "|                True|           1899|\n",
      "|           Fida Daar|              3|\n",
      "|            farfalle|             41|\n",
      "+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#need to address these wonky values as well as the fact that False and FALSE/True and TRUE are different\n",
    "from pyspark.sql.functions import when\n",
    "YouTube_df = YouTube_df.withColumn(\"comments_disabled\", when(YouTube_df.comments_disabled == \"FALSE\",\"False\") \\\n",
    "      .when(YouTube_df.comments_disabled == \"TRUE\",\"True\") \\\n",
    "      .otherwise(YouTube_df.comments_disabled))\n",
    "YouTube_df.groupBy(\"comments_disabled\").agg(F.count(\"video_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that all of the desired values are False and True instead of all caps, let's filter this bad boy\n",
    "YouTube_df2 = YouTube_df.filter(YouTube_df.comments_disabled == 'True')\n",
    "YouTube_df3 = YouTube_df.filter(YouTube_df.comments_disabled == 'False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTube_final = YouTube_df2.union(YouTube_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+\n",
      "|comments_disabled|count(video_id)|\n",
      "+-----------------+---------------+\n",
      "|            False|         118847|\n",
      "|             True|           1899|\n",
      "+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "YouTube_final.groupBy(\"comments_disabled\").agg(F.count(\"video_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time to do the same for ratings disabled\n",
    "YouTube_final.groupBy(\"ratings_disabled\").agg(F.count(\"video_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTube_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting some summary statistics for the numeric variables \n",
    "youtube_summary = YouTube_final.select(\"views\",\"likes\",\"dislikes\",\"comment_count\").describe()\n",
    "youtube_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "here we're trying to figure out where there are missing values. Appears to be a lot of missing values but\n",
    "it doesn't matter a ton to us because we're going to focus on the description first\n",
    "\n",
    "Code adapted from: https://sparkbyexamples.com/pyspark/pyspark-find-count-of-null-none-nan-values/\n",
    "'''\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "YouTube_final.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in YouTube_final.columns]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Casting strings as numeric. Not sure why this was giving me issues like this but casting so we can do\n",
    "correlation matrix\n",
    "'''\n",
    "Numeric_only  = YouTube_final.select(\"views\",\"likes\",\"dislikes\",\"comment_count\")\n",
    "Numeric_only = Numeric_only.withColumn(\"views\",col(\"views\").cast(\"int\"))\n",
    "Numeric_only = Numeric_only.withColumn(\"likes\",col(\"likes\").cast(\"int\"))\n",
    "Numeric_only = Numeric_only.withColumn(\"dislikes\",col(\"dislikes\").cast(\"int\"))\n",
    "Numeric_only = Numeric_only.withColumn(\"comment_count\",col(\"views\").cast(\"int\"))\n",
    "Numeric_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Got the correlation matrix for our numeric features.\n",
    "https://stackoverflow.com/questions/52214404/how-to-get-the-correlation-matrix-of-a-pyspark-data-frame\n",
    "We can see here that numeric features are pretty heavily correlated. \n",
    "'''\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# convert to vector column first\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols=Numeric_only.columns, outputCol=vector_col)\n",
    "df_vector = assembler.transform(Numeric_only).select(vector_col)\n",
    "\n",
    "# get correlation matrix\n",
    "matrix = Correlation.corr(df_vector, vector_col)\n",
    "matrix.collect()[0][\"pearson({})\".format(vector_col)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
